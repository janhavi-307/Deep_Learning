{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UAkzTTWfLXrb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('imdb_top_1000.csv')\n",
        "\n",
        "print(data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcA8Tc8_jSjd",
        "outputId": "7b3c9abe-aad7-457c-cf76-ab0b6eb4a7b7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Poster_Link  \\\n",
            "0    https://m.media-amazon.com/images/M/MV5BMDFkYT...   \n",
            "1    https://m.media-amazon.com/images/M/MV5BM2MyNj...   \n",
            "2    https://m.media-amazon.com/images/M/MV5BMTMxNT...   \n",
            "3    https://m.media-amazon.com/images/M/MV5BMWMwMG...   \n",
            "4    https://m.media-amazon.com/images/M/MV5BMWU4N2...   \n",
            "..                                                 ...   \n",
            "995  https://m.media-amazon.com/images/M/MV5BNGEwMT...   \n",
            "996  https://m.media-amazon.com/images/M/MV5BODk3Yj...   \n",
            "997  https://m.media-amazon.com/images/M/MV5BM2U3Yz...   \n",
            "998  https://m.media-amazon.com/images/M/MV5BZTBmMj...   \n",
            "999  https://m.media-amazon.com/images/M/MV5BMTY5OD...   \n",
            "\n",
            "                 Series_Title Released_Year Certificate  Runtime  \\\n",
            "0    The Shawshank Redemption          1994           A  142 min   \n",
            "1               The Godfather          1972           A  175 min   \n",
            "2             The Dark Knight          2008          UA  152 min   \n",
            "3      The Godfather: Part II          1974           A  202 min   \n",
            "4                12 Angry Men          1957           U   96 min   \n",
            "..                        ...           ...         ...      ...   \n",
            "995    Breakfast at Tiffany's          1961           A  115 min   \n",
            "996                     Giant          1956           G  201 min   \n",
            "997     From Here to Eternity          1953      Passed  118 min   \n",
            "998                  Lifeboat          1944         NaN   97 min   \n",
            "999              The 39 Steps          1935         NaN   86 min   \n",
            "\n",
            "                        Genre  IMDB_Rating  \\\n",
            "0                       Drama          9.3   \n",
            "1                Crime, Drama          9.2   \n",
            "2        Action, Crime, Drama          9.0   \n",
            "3                Crime, Drama          9.0   \n",
            "4                Crime, Drama          9.0   \n",
            "..                        ...          ...   \n",
            "995    Comedy, Drama, Romance          7.6   \n",
            "996            Drama, Western          7.6   \n",
            "997       Drama, Romance, War          7.6   \n",
            "998                Drama, War          7.6   \n",
            "999  Crime, Mystery, Thriller          7.6   \n",
            "\n",
            "                                              Overview  Meta_score  \\\n",
            "0    Two imprisoned men bond over a number of years...        80.0   \n",
            "1    An organized crime dynasty's aging patriarch t...       100.0   \n",
            "2    When the menace known as the Joker wreaks havo...        84.0   \n",
            "3    The early life and career of Vito Corleone in ...        90.0   \n",
            "4    A jury holdout attempts to prevent a miscarria...        96.0   \n",
            "..                                                 ...         ...   \n",
            "995  A young New York socialite becomes interested ...        76.0   \n",
            "996  Sprawling epic covering the life of a Texas ca...        84.0   \n",
            "997  In Hawaii in 1941, a private is cruelly punish...        85.0   \n",
            "998  Several survivors of a torpedoed merchant ship...        78.0   \n",
            "999  A man in London tries to help a counter-espion...        93.0   \n",
            "\n",
            "                 Director              Star1              Star2  \\\n",
            "0          Frank Darabont        Tim Robbins     Morgan Freeman   \n",
            "1    Francis Ford Coppola      Marlon Brando          Al Pacino   \n",
            "2       Christopher Nolan     Christian Bale       Heath Ledger   \n",
            "3    Francis Ford Coppola          Al Pacino     Robert De Niro   \n",
            "4            Sidney Lumet        Henry Fonda        Lee J. Cobb   \n",
            "..                    ...                ...                ...   \n",
            "995         Blake Edwards     Audrey Hepburn     George Peppard   \n",
            "996        George Stevens   Elizabeth Taylor        Rock Hudson   \n",
            "997        Fred Zinnemann     Burt Lancaster   Montgomery Clift   \n",
            "998      Alfred Hitchcock  Tallulah Bankhead        John Hodiak   \n",
            "999      Alfred Hitchcock       Robert Donat  Madeleine Carroll   \n",
            "\n",
            "              Star3           Star4  No_of_Votes        Gross  \n",
            "0        Bob Gunton  William Sadler      2343110   28,341,469  \n",
            "1        James Caan    Diane Keaton      1620367  134,966,411  \n",
            "2     Aaron Eckhart   Michael Caine      2303232  534,858,444  \n",
            "3     Robert Duvall    Diane Keaton      1129952   57,300,000  \n",
            "4     Martin Balsam    John Fiedler       689845    4,360,000  \n",
            "..              ...             ...          ...          ...  \n",
            "995   Patricia Neal     Buddy Ebsen       166544          NaN  \n",
            "996      James Dean   Carroll Baker        34075          NaN  \n",
            "997    Deborah Kerr      Donna Reed        43374   30,500,000  \n",
            "998   Walter Slezak  William Bendix        26471          NaN  \n",
            "999  Lucie Mannheim  Godfrey Tearle        51853          NaN  \n",
            "\n",
            "[1000 rows x 16 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TxUUNjCm6Wv",
        "outputId": "91112b9d-86d8-4e05-d101-5fae02e93354"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RejS2u6xm_Si",
        "outputId": "d3628d52-22c6-4e46-c20f-6c1ffc448afc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"shan't\", \"weren't\", 'ours', \"you'd\", 'himself', 'doing', \"they're\", 'when', 'mustn', 'll', 'of', 'very', 'whom', \"didn't\", 'should', 'more', 'here', 'how', 'herself', 'are', 'have', 'as', 'most', 'which', 'after', 'or', 'wouldn', 'down', 'in', 'if', 'ain', \"needn't\", 'above', 'do', 'does', 'i', \"it's\", 'itself', \"i've\", 'me', 'shouldn', 'ma', \"you'll\", 's', 'what', 'm', 'both', 'same', 'our', \"won't\", 'y', 'was', 'myself', \"they'll\", 'theirs', 'you', 'such', 'my', \"that'll\", \"it'll\", 'ourselves', 'own', 'shan', 'so', \"he's\", 'having', 'and', 'did', 'why', \"don't\", 'all', 'about', 'other', 'will', \"you're\", 'below', 'them', 'won', 'then', \"i'd\", 'can', \"hadn't\", 'weren', 'each', 'up', 'under', 'on', 'too', 'only', \"should've\", \"we've\", \"aren't\", 'the', 'wasn', 'for', \"hasn't\", 'over', 'be', 'mightn', 'than', 'during', 'been', 'not', \"haven't\", 'while', 'further', 'against', 'some', 'were', 'his', 'd', 'themselves', 'to', 'its', 'needn', 'again', 'there', 'him', 'just', 'don', 'these', 'with', 'o', 'out', 'this', 'through', 'yours', 'doesn', 'by', \"i'm\", 'being', 'haven', 'any', 'off', \"he'll\", 'a', 'no', 're', 'their', 'he', \"we'll\", 'she', 'we', \"wouldn't\", 'am', \"isn't\", 've', \"she'd\", 'until', 'yourself', \"mustn't\", 'is', 'because', 'at', 'your', 'hadn', \"they've\", 'those', \"wasn't\", 'aren', 'didn', 'from', 'now', 'hers', \"we're\", \"couldn't\", \"he'd\", 'between', 'but', \"doesn't\", 'had', 'where', \"we'd\", 'has', 'that', 'an', 'couldn', 'hasn', \"it'd\", 'who', 'into', 'isn', 'before', 'few', \"you've\", \"they'd\", 'yourselves', \"i'll\", 'they', 'once', \"she's\", 'nor', 'it', \"she'll\", \"shouldn't\", \"mightn't\", 'her', 't'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stops = set(stopwords.words('english'))\n",
        ""
      ],
      "metadata": {
        "id": "uBfcTo_lnDNQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    df = pd.read_csv('imdb_top_1000.csv')\n",
        "    x_data = df['Overview']       # Reviews/Input\n",
        "    y_data = df['IMDB_Rating']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n",
        "\n",
        "print('Overview')\n",
        "print(x_data, '\\n')\n",
        "print('IMDB_Rating')\n",
        "print(y_data)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS_HcDzwnSr9",
        "outputId": "93698898-c49f-40d7-a468-8da391322626"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overview\n",
            "0      [two, imprisoned, men, bond, number, years, fi...\n",
            "1      [an, organized, crime, dynasty, aging, patriar...\n",
            "2      [when, menace, known, joker, wreaks, havoc, ch...\n",
            "3      [the, early, life, career, vito, corleone, new...\n",
            "4      [a, jury, holdout, attempts, prevent, miscarri...\n",
            "                             ...                        \n",
            "995    [a, young, new, york, socialite, becomes, inte...\n",
            "996    [sprawling, epic, covering, life, texas, cattl...\n",
            "997    [in, hawaii, private, cruelly, punished, boxin...\n",
            "998    [several, survivors, torpedoed, merchant, ship...\n",
            "999    [a, man, london, tries, help, counter, espiona...\n",
            "Name: Overview, Length: 1000, dtype: object \n",
            "\n",
            "IMDB_Rating\n",
            "0      9.3\n",
            "1      9.2\n",
            "2      9.0\n",
            "3      9.0\n",
            "4      9.0\n",
            "      ... \n",
            "995    7.6\n",
            "996    7.6\n",
            "997    7.6\n",
            "998    7.6\n",
            "999    7.6\n",
            "Name: IMDB_Rating, Length: 1000, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
        "\n",
        "print('Train Set')\n",
        "print(x_train, '\\n')\n",
        "print(x_test, '\\n')\n",
        "print('Test Set')\n",
        "print(y_train, '\\n')\n",
        "print(y_test)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RVX8UcaqPpo",
        "outputId": "f95226e7-785a-4d32-c737-7b7a9305861d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set\n",
            "177    [after, family, murdered, notorious, ruthless,...\n",
            "291    [in, fear, violence, escalate, people, algiers...\n",
            "269    [travis, henderson, aimless, drifter, missing,...\n",
            "929    [at, turning, point, life, former, tennis, pro...\n",
            "474    [when, louis, bloom, con, man, desperate, work...\n",
            "                             ...                        \n",
            "639    [sixteen, year, old, lilja, friend, young, boy...\n",
            "748    [as, harvard, student, mark, zuckerberg, creat...\n",
            "751    [james, bond, loyalty, m, tested, past, comes,...\n",
            "358    [a, precocious, outspoken, iranian, girl, grow...\n",
            "470    [a, national, manhunt, ordered, rebellious, ki...\n",
            "Name: Overview, Length: 800, dtype: object \n",
            "\n",
            "450    [two, men, attempt, prove, committed, perfect,...\n",
            "440    [powerful, unethical, broadway, columnist, j, ...\n",
            "367    [a, rat, cook, makes, unusual, alliance, young...\n",
            "66     [in, distant, future, small, waste, collecting...\n",
            "170    [when, two, girls, move, country, near, ailing...\n",
            "                             ...                        \n",
            "709    [dunson, leads, cattle, drive, culmination, ye...\n",
            "945    [the, eccentric, members, dysfunctional, famil...\n",
            "810    [pianist, david, helfgott, driven, father, tea...\n",
            "617    [a, modern, day, musical, busker, immigrant, e...\n",
            "377    [a, frustrated, son, tries, determine, fact, f...\n",
            "Name: Overview, Length: 200, dtype: object \n",
            "\n",
            "Test Set\n",
            "177    8.2\n",
            "291    8.1\n",
            "269    8.1\n",
            "929    7.6\n",
            "474    7.9\n",
            "      ... \n",
            "639    7.8\n",
            "748    7.7\n",
            "751    7.7\n",
            "358    8.0\n",
            "470    7.9\n",
            "Name: IMDB_Rating, Length: 800, dtype: float64 \n",
            "\n",
            "450    8.0\n",
            "440    8.0\n",
            "367    8.0\n",
            "66     8.4\n",
            "170    8.2\n",
            "      ... \n",
            "709    7.8\n",
            "945    7.6\n",
            "810    7.7\n",
            "617    7.8\n",
            "377    8.0\n",
            "Name: IMDB_Rating, Length: 200, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length():\n",
        "    review_length = []\n",
        "    for review in x_train:\n",
        "        review_length.append(len(review))\n",
        "\n",
        "    return int(np.ceil(np.mean(review_length)))\n",
        "\n"
      ],
      "metadata": {
        "id": "IqVTr95gqa-D"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE REVIEW\n",
        "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
        "token.fit_on_texts(x_train)\n",
        "x_train = token.texts_to_sequences(x_train)\n",
        "x_test = token.texts_to_sequences(x_test)\n",
        "\n",
        "max_length = get_max_length()\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
        "\n",
        "print('Encoded X Train\\n', x_train, '\\n')\n",
        "print('Encoded X Test\\n', x_test, '\\n')\n",
        "print('Maximum review length: ', max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBNhl1Q1qhpT",
        "outputId": "e361e0e2-ddbf-4e6d-d183-26d0d9a9926b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded X Train\n",
            " [[  20   11  384 ...  723  522    0]\n",
            " [   8  524  166 ...    0    0    0]\n",
            " [1817 1818 1819 ...   11    0    0]\n",
            " ...\n",
            " [ 346  164 1259 ...   18  200  210]\n",
            " [   1 1792 4781 ...    0    0    0]\n",
            " [   1  501 1805 ... 4784    0    0]] \n",
            "\n",
            "Encoded X Test\n",
            " [[   6   70  147 ...    0    0    0]\n",
            " [ 379 1247  607 ...    0    0    0]\n",
            " [   1  281 2510 ...    0    0    0]\n",
            " ...\n",
            " [1256 2706   30 ...    0    0    0]\n",
            " [   1  832   71 ...    0    0    0]\n",
            " [   1  657   28 ...    0    0    0]] \n",
            "\n",
            "Maximum review length:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ARCHITECTURE\n",
        "EMBED_DIM = 32\n",
        "LSTM_OUT = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
        "model.add(LSTM(LSTM_OUT))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "dnPbdAWOqnIN",
        "outputId": "2be68ad5-4803-486d-bea9-545775bd9711"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'models/LSTM.h5',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "LcQyKlibqv7V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rgdeOOlqyVJ",
        "outputId": "8ba0eb12-7dc2-434b-f87c-6ac7c104f5e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 0.3642\n",
            "Epoch 1: accuracy improved from -inf to 0.00000, saving model to models/LSTM.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.2750\n",
            "Epoch 2/5\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: -2.3468\n",
            "Epoch 2: accuracy did not improve from 0.00000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: -2.5830\n",
            "Epoch 3/5\n",
            "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: -9.4972\n",
            "Epoch 3: accuracy did not improve from 0.00000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: -10.9678\n",
            "Epoch 4/5\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0000e+00 - loss: -34.1413\n",
            "Epoch 4: accuracy did not improve from 0.00000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: -35.5812\n",
            "Epoch 5/5\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: -56.3574\n",
            "Epoch 5: accuracy did not improve from 0.00000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: -56.9725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cce143275f0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(x_test, batch_size=128)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kBS4-ekrqI6",
        "outputId": "5397d960-59f1-44f2-93e9-00a27367fc4e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "true = 0\n",
        "for i, y in enumerate(y_test):\n",
        "    if y == y_pred[i]:\n",
        "        true += 1\n",
        "\n",
        "print('Correct Prediction: {}'.format(true))\n",
        "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
        "print('Accuracy: {}'.format(true/len(y_pred)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nr_pcQHq4au",
        "outputId": "fda1d25f-210a-45ba-dcd7-5303386cc4fe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct Prediction: 0\n",
            "Wrong Prediction: 200\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('models/LSTM.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex7zEG9Mr5EN",
        "outputId": "01b752b5-8d08-4445-a306-ac439ed2c6cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review = str(input('Movie Review: '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fB4qco2sABY",
        "outputId": "7d7e6f46-58b5-4084-cb22-d5f82353caef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie Review: good movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re   # ← add this line\n",
        "\n",
        "# Pre-process input\n",
        "regex = re.compile(r'[^a-zA-Z\\s]')\n",
        "review = regex.sub('', review)\n",
        "print('Cleaned:', review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUuzTriVwa8Z",
        "outputId": "cced0f7a-fed4-49a0-a9b7-8c7743d1ff61"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned: good movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = review.split(' ')\n",
        "filtered = [w for w in words if w not in english_stops]\n",
        "filtered = ' '.join(filtered)\n",
        "filtered = [filtered.lower()]\n",
        "\n",
        "print('Filtered: ', filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcDqJcOiweyS",
        "outputId": "4b4df6d8-2d5e-41cd-c98c-33b35d7786f7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered:  ['good movie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_words = token.texts_to_sequences(filtered)\n",
        "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "print(tokenize_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwJF9QUPwliO",
        "outputId": "de7fe1dc-672f-41fa-8713-f9bffda8c3aa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[503 203   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = loaded_model.predict(tokenize_words)\n",
        "print(result)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HdM78pnwrcU",
        "outputId": "e3449747-6cf1-4abe-f2c2-114d49477d1a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cce1deb7740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
            "[[0.6833575]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if result >= 0.7:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWpx8EOxw0zF",
        "outputId": "2bc77e07-220d-4864-962a-092b5fe6f6f4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    }
  ]
}